{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Revised Code\n",
    "\n",
    "The purpose of this notebook is to make sure my revised PoeML app actually works.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_and_file_names():\n",
    "    root_dir = \"/Users/ctoews/Documents/Insight/app_demo\"\n",
    "    api_dir = \"/flaskexample/static/api\"\n",
    "    pkl_dir = \"/flaskexample/static/pkl\"\n",
    "    api_file = \"/MyFirstProject-76680dcd1ad6.json\"\n",
    "    poem_file = \"df1_smallpoems.pkl\"\n",
    "    vec_file = \"df1_vecs.pkl\"\n",
    "    vectorizer_file = \"d1_vectorizer_replacement.pkl\"\n",
    "    return root_dir, api_dir, pkl_dir, api_file, poem_file, vec_file, vectorizer_file\n",
    "\n",
    " \n",
    "def get_runtime_parameters():\n",
    "    n_matches_per_photo = 3 # maximum number of images to return\n",
    "    lam = .1        # regularization parameter\n",
    "    batch = False    # use averaging technique to handle multiple images\n",
    "    return n_matches_per_photo, lam, batch\n",
    "\n",
    "\n",
    "def get_pkl_files(root_dir,pkl_dir,poem_file,vec_file,vectorizer_file):\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    df_poems = pd.read_pickle(root_dir + pkl_dir + '/' + poem_file)\n",
    "    df_vecs =   pd.read_pickle(root_dir + pkl_dir + '/' + vec_file)\n",
    "    vectorizer = pickle.load( open( root_dir + pkl_dir + '/' + vectorizer_file, \"rb\" ) )\n",
    "    return df_poems, df_vecs, vectorizer\n",
    "\n",
    "\n",
    "\n",
    "def get_stopwords():\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    STOPLIST = stopwords.words('english')\n",
    "    SYMBOLS = \" \".join(string.punctuation).split(\" \") + \\\n",
    "              [\"-----\", \"--\", \"---\", \"...\", \"“\", \"”\", \"'s\"] + list(string.digits)\n",
    "    return STOPLIST, SYMBOLS\n",
    "\n",
    "\n",
    "def tokenizeText(sample):\n",
    "\n",
    "    import spacy\n",
    "    global STOPLIST\n",
    "    global SYMBOLS\n",
    "\n",
    "    # get the tokens using spaCy\n",
    "    tokens = parser(sample)\n",
    "\n",
    "    # lemmatize\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip()\n",
    "                      if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "\n",
    "    STOPWORDS, SYMBOLS = get_stopwords()\n",
    "\n",
    "    # stoplist the tokens\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "\n",
    "    # stoplist symbols\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "\n",
    "    # remove large strings of whitespace\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "# extract image urls from information in photoset object (returned from Flickr api call)\n",
    "def assemble_urls(photoset):\n",
    "    urls = []\n",
    "    for photo in photoset['photoset']['photo']:\n",
    "        url = \"https://farm\" + str(photo['farm']) + \".staticflickr.com/\" + photo['server'] + \"/\" + \\\n",
    "              photo['id'] + \"_\" + photo['secret'] + \".jpg\"\n",
    "        urls.append(url)\n",
    "    return urls\n",
    "\n",
    "\n",
    "\n",
    "# extact userid and albumid from Flickr album url (used to form image urls)\n",
    "def parse_url(url):\n",
    "\n",
    "    import re\n",
    "\n",
    "    try:\n",
    "        userid = re.search('photos/(.+?)/', url).group(1)\n",
    "    except AttributeError:\n",
    "        # AAA, ZZZ not found in the original string\n",
    "        userid = '' # apply your error handling\n",
    "\n",
    "    try:\n",
    "        albumid = re.search('albums/(.*)', url).group(1)\n",
    "    except AttributeError:\n",
    "        albumid = '' # apply your error handling\n",
    "\n",
    "    return userid, albumid\n",
    "\n",
    "\n",
    "def get_flickr_urls(url):\n",
    "\n",
    "    import flickrapi\n",
    "\n",
    "    #import flickr_keys\n",
    "    api_key = u'37528c980c419716e0879a417ef8211c'\n",
    "    api_secret = u'41075654a535c203'\n",
    "\n",
    "    # establish connection\n",
    "    flickr = flickrapi.FlickrAPI(api_key, api_secret, format='parsed-json')\n",
    "\n",
    "    # extract user and album id\n",
    "    userid, albumid = parse_url(url)\n",
    "\n",
    "    #fetch album info\n",
    "    albuminfo  = flickr.photosets.getPhotos(user_id=userid,photoset_id=albumid)\n",
    "\n",
    "    # extract individual photo urls\n",
    "    photo_urls = assemble_urls(albuminfo)\n",
    "\n",
    "    return photo_urls\n",
    "\n",
    "\n",
    "\n",
    "def get_photo_urls(url):\n",
    "    # input could be a Flickr photo album url\n",
    "    if 'www.flickr.com/photos/' in url:\n",
    "        photo_urls = get_flickr_urls(url)\n",
    "\n",
    "    # or a list of image jpegs\n",
    "    else:\n",
    "        photo_urls = url.split(',')\n",
    "\n",
    "    return photo_urls\n",
    "\n",
    "\n",
    "\n",
    "# connect to google api\n",
    "def explicit(root_dir, api_dir, api_file):\n",
    "    from google.cloud import storage\n",
    "    # Explicitly use service account credentials by specifying the private key\n",
    "    # file.\n",
    "    storage_client = storage.Client.from_service_account_json(\n",
    "        root_dir + api_dir + '/' + api_file)\n",
    "\n",
    "    # Make an authenticated API request\n",
    "    buckets = list(storage_client.list_buckets())\n",
    "    print(buckets)\n",
    "\n",
    "\n",
    "\n",
    "def get_labels_for_remote_images(photo_urls, root_dir, api_dir, api_file):\n",
    "    import os\n",
    "    from google.cloud import vision\n",
    "    from google.cloud.vision import types\n",
    "    import pandas as pd\n",
    "    # authenticate\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \\\n",
    "        root_dir+ api_dir + '/' + api_file\n",
    "    explicit(root_dir, api_dir, api_file)\n",
    "\n",
    "    # connect to Google api\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    image = types.Image()\n",
    "\n",
    "    # feed photo url to Google, extract label\n",
    "    all_labels = []\n",
    "    for url in photo_urls:\n",
    "        image.source.image_uri = url\n",
    "        response = client.label_detection(image=image)\n",
    "        labels = response.label_annotations\n",
    "        these_labels = ''\n",
    "        for label in labels:\n",
    "            these_labels += (label.description + ' ')\n",
    "        all_labels.append(these_labels)\n",
    "\n",
    "    # store labels as dataframe\n",
    "    df_all_labels = pd.DataFrame({'keywords':all_labels,'url':photo_urls})\n",
    "\n",
    "    # eliminate any photo that came back with zero labels\n",
    "    df_all_labels = df_all_labels.loc[df_all_labels.keywords.apply(lambda x: len(x))!=0]\n",
    "\n",
    "    return df_all_labels\n",
    "\n",
    "\n",
    "def get_labels_for_local_images(photo_urls, root_dir, api_dir, api_file):\n",
    "    \"\"\"This function will need to be changed...doesn't currently work\"\"\"\n",
    "    import os\n",
    "    from google.cloud import vision\n",
    "    from google.cloud.vision import types\n",
    "\n",
    "    # authenticate\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \\\n",
    "        root_dir + api_dir + '/' + api_file\n",
    "\n",
    "    explicit(root_dir, api_dir, api_file)\n",
    "\n",
    "    # connect to Google api\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    image = types.Image()\n",
    "\n",
    "    # feed photo url to Google, extract label\n",
    "    all_labels = []\n",
    "    for url in photo_urls:\n",
    "        image.source.image_uri = url\n",
    "        response = client.label_detection(image=image)\n",
    "        labels = response.label_annotations\n",
    "        these_labels = ''\n",
    "        for label in labels:\n",
    "            these_labels += (label.description + ' ')\n",
    "        all_labels.append(these_labels)\n",
    "\n",
    "    # store labels as dataframe\n",
    "    all_labels = pd.DataFrame(all_labels,columns=['labels'])\n",
    "\n",
    "    return all_labels\n",
    "\n",
    "\n",
    "\n",
    "def extract_n_top_words_from_poem(poem_vector,feature_names):\n",
    "    import numpy as np\n",
    "\n",
    "    # adjust as necessary\n",
    "    ntopwords = 10\n",
    "\n",
    "    # rank keywords by tf-idf weight\n",
    "    indices = poem_vector.indices\n",
    "    rank_idx = poem_vector.data.argsort()[:-ntopwords:-1]\n",
    "\n",
    "    # form list of such words and return it, along with weights\n",
    "    keywords = [feature_names[indices[i]] for i in rank_idx]\n",
    "    weights = [poem_vector.data[i] for i in rank_idx]\n",
    "\n",
    "    return keywords, np.array(weights)\n",
    "\n",
    "\n",
    "# transform the image labels with the vectorizer\n",
    "def weight_labels(df_all_labels, vectorizer):\n",
    "    import spacy\n",
    "    import pandas as pd\n",
    "\n",
    "    image_words = []\n",
    "    image_weights = []\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # the vectorizer seems to need to have access to the parser, probably for the tokenizing step\n",
    "    parser = spacy.load('en')\n",
    "\n",
    "    for row in vectorizer.transform(df_all_labels['keywords'].tolist()):\n",
    "        kw, wt = extract_n_top_words_from_poem(row,feature_names)\n",
    "        image_words.append(kw)\n",
    "        image_weights.append(wt)\n",
    "\n",
    "    df_images = df_all_labels\n",
    "    df_images['keywords'] = image_words\n",
    "    df_images['weights'] = image_weights\n",
    "\n",
    "    # eliminate any photo that came back with zero labels\n",
    "    df_images = df_images.loc[df_images.keywords.apply(lambda x: len(x))!=0]\n",
    "\n",
    "    return df_images\n",
    "\n",
    "\n",
    "\n",
    "def images2vec(df_images):\n",
    "    import spacy\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # load parser, to be used with vectorizer\n",
    "    parser = spacy.load('en')\n",
    "\n",
    "    image_vectors = np.zeros((len(df_images),384))\n",
    "    j=0\n",
    "    for row in df_images.itertuples():\n",
    "        keywords = row.keywords\n",
    "        weights = row.weights\n",
    "        vecs = np.zeros((len(keywords),384))\n",
    "        i = 0\n",
    "        for k in keywords:\n",
    "            vecs[i,:] = parser(k).vector\n",
    "            i+=1\n",
    "        image_vectors[j,:]=np.dot(weights,vecs)\n",
    "        j+=1\n",
    "\n",
    "    return image_vectors\n",
    "\n",
    "\n",
    "\n",
    "def find_best_match(image_vectors, poem_vectors, image_sentiment, poem_sentiment,n_matches_per_photo=3,batch=True,lam=.1):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    # find poem that maximizes a sentiment-regularized objective function\n",
    "    if batch:\n",
    "        image_vectors = np.mean(image_vectors,axis=0).reshape(1,384)\n",
    "        image_sentiment = [np.mean(image_sentiment)]\n",
    "\n",
    "    # assess the cosine similarity for each image/poem pair\n",
    "    sim = cosine_similarity(image_vectors,poem_vectors)\n",
    "\n",
    "    # also calculate the difference in sentiment score\n",
    "    dif = np.array([np.abs((im_s - poem_sentiment)) for im_s in image_sentiment])\n",
    "\n",
    "    # the net score is a weighted difference\n",
    "    net = sim - lam*dif\n",
    "\n",
    "    ix = net.argsort(axis=1)[:,:-n_matches_per_photo-1:-1]\n",
    "    scores = np.array([ list(net[i,ix[i,:]]) for i in range(len(ix))])\n",
    "\n",
    "    return ix, scores\n",
    "\n",
    "\n",
    "\n",
    "def gather_results(ix,scores,df_images,df_poems,photo_urls):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    # gather top N poems (for each picture, or for the \"average\" picture)\n",
    "    results = pd.DataFrame([ df_poems.loc[ix[i,:],'poem'].tolist() for i in range(len(ix))],\\\n",
    "                           columns = [str(i) for i in range(1,ix.shape[1]+1)])\n",
    "\n",
    "    # collect image urls and keywords\n",
    "    if len(results) == len(df_images):\n",
    "        results[['url','keywords','weights','sentiment']] = \\\n",
    "        df_images[['url','keywords','weights','sentiment']]\n",
    "\n",
    "    # if in batchmode, collect images with the most keywords\n",
    "    else:\n",
    "        ix = np.argmax(df_images.keywords.apply(lambda x: len(x)))\n",
    "        results['url']= df_images.loc[ix,'url']\n",
    "        results['keywords']= [df_images.loc[ix,'keywords']]\n",
    "        results['weights']=[df_images.loc[ix,'weights']]\n",
    "        results['sentiment']= df_images.loc[ix,'sentiment']\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def ModelIt(url):\n",
    "\n",
    "    from PIL import Image, ImageDraw\n",
    "    import pandas as pd\n",
    "    import spacy\n",
    "    import numpy as np\n",
    "    from textblob import TextBlob\n",
    "\n",
    "    # load up path and file names, as well as runtime parameters\n",
    "    root_dir, api_dir, pkl_dir, api_file, poem_file, vec_file, vectorizer_file =\\\n",
    "        get_path_and_file_names()\n",
    "    n_matches_per_photo, lam, batch = get_runtime_parameters()\n",
    "\n",
    "    # some of the larger data structures are stored in binary form, to expedite runtime\n",
    "    df_poems, df_vecs, vectorizer = get_pkl_files(root_dir,pkl_dir,poem_file,vec_file,vectorizer_file)\n",
    "    poem_vectors = df_vecs.values\n",
    "\n",
    "    # Set the variable \"photo_urls\", which is a list of urls of all images\n",
    "    photo_urls = get_photo_urls(url)\n",
    "\n",
    "    # Connect to Google-Cloud-Vision API and extract labels for each image\n",
    "    df_all_labels = get_labels_for_remote_images(photo_urls, root_dir, api_dir, api_file)\n",
    "\n",
    "    # weight the keywords by the vectorizer used to process the poetry text\n",
    "    df_images = weight_labels(df_all_labels, vectorizer)\n",
    "\n",
    "    # append sentiment analysis for each image\n",
    "    df_images['sentiment'] = [TextBlob(' '.join(x)).sentiment[0] for x in df_images.keywords]\n",
    "\n",
    "    # if after extracting and weighting labels, nothing remains, exit gracefully\n",
    "    if len(df_images)==0:\n",
    "        return -1\n",
    "\n",
    "    # otherwise, embed image vectors via word2vec\n",
    "    image_vectors = images2vec(df_images)\n",
    "\n",
    "    # return sorted scores\n",
    "    ix, scores = find_best_match(image_vectors, poem_vectors, df_images['sentiment'], df_poems['sentiment'],batch=batch)\n",
    "\n",
    "    # gather all relevant info into a dataframe\n",
    "    results = gather_results(ix,scores,df_images,df_poems,photo_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "\n",
    "# load up path and file names, as well as runtime parameters\n",
    "root_dir, api_dir, pkl_dir, api_file, poem_file, vec_file, vectorizer_file =\\\n",
    "get_path_and_file_names()\n",
    "n_matches_per_photo, lam, batch = get_runtime_parameters()\n",
    "photo_urls = get_photo_urls(url)\n",
    "df_all_labels = get_labels_for_remote_images(photo_urls, root_dir, api_dir, api_file)\n",
    "df_images = weight_labels(df_all_labels, vectorizer)\n",
    "df_images['sentiment'] = [TextBlob(' '.join(x)).sentiment[0] for x in df_images.keywords]\n",
    "image_vectors = images2vec(df_images)\n",
    "ix, scores = find_best_match(image_vectors, poem_vectors, df_images['sentiment'], df_poems['sentiment'],batch=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = gather_results(ix,scores,df_images,df_poems,photo_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poems.loc[2719,'keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url=\"https://www.flickr.com/photos/138072685@N02/albums/72157691244283801\"\n",
    "url = \"http://sites.psu.edu/mgeppingerpassionblog/wp-content/uploads/sites/32731/2015/09/roads-diverging.jpg,https://i.ytimg.com/vi/opKg3fyqWt4/hqdefault.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ModelIt(url)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test newly revised code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file contains the backend to PoeML\n",
    "\n",
    "def get_path_and_file_names():\n",
    "    #root_dir = \"/home/ubuntu/app_demo\"\n",
    "    root_dir = \"/Users/ctoews/Documents/Insight/app_demo\"\n",
    "    api_dir = \"/flaskexample/static/api\"\n",
    "    pkl_dir = \"/flaskexample/static/pkl\"\n",
    "    api_file = \"/MyFirstProject-76680dcd1ad6.json\"\n",
    "    poem_file = \"df1_smallpoems.pkl\"\n",
    "    vec_file = \"df1_vecs.pkl\"\n",
    "    vectorizer_file = \"d1_vectorizer_replacement.pkl\"\n",
    "    return root_dir, api_dir, pkl_dir, api_file, poem_file, vec_file, vectorizer_file\n",
    "\n",
    "\n",
    "# def get_runtime_parameters():\n",
    "#     n_matches_per_photo = 3 # maximum number of images to return\n",
    "#     lam = .1        # regularization parameter\n",
    "#     batch = True    # use averaging technique to handle multiple images\n",
    "#     return n_matches_per_photo, lam, batch\n",
    "\n",
    "\n",
    "def get_pkl_files(root_dir,pkl_dir,poem_file,vec_file,vectorizer_file):\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    df_poems = pd.read_pickle(root_dir + pkl_dir + '/' + poem_file)\n",
    "    df_vecs =   pd.read_pickle(root_dir + pkl_dir + '/' + vec_file)\n",
    "    vectorizer = pickle.load( open( root_dir + pkl_dir + '/' + vectorizer_file, \"rb\" ) )\n",
    "    return df_poems, df_vecs, vectorizer\n",
    "\n",
    "\n",
    "\n",
    "def get_stopwords():\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    STOPLIST = stopwords.words('english')\n",
    "    SYMBOLS = \" \".join(string.punctuation).split(\" \") + \\\n",
    "              [\"-----\", \"--\", \"---\", \"...\", \"“\", \"”\", \"'s\"] + list(string.digits)\n",
    "    return STOPLIST, SYMBOLS\n",
    "\n",
    "\n",
    "def tokenizeText(sample):\n",
    "\n",
    "    import spacy\n",
    "    global STOPLIST\n",
    "    global SYMBOLS\n",
    "\n",
    "    # get the tokens using spaCy\n",
    "    tokens = parser(sample)\n",
    "\n",
    "    # lemmatize\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip()\n",
    "                      if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "\n",
    "    STOPWORDS, SYMBOLS = get_stopwords()\n",
    "\n",
    "    # stoplist the tokens\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "\n",
    "    # stoplist symbols\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "\n",
    "    # remove large strings of whitespace\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "# extract image urls from information in photoset object (returned from Flickr api call)\n",
    "def assemble_urls(photoset):\n",
    "    urls = []\n",
    "    for photo in photoset['photoset']['photo']:\n",
    "        url = \"https://farm\" + str(photo['farm']) + \".staticflickr.com/\" + photo['server'] + \"/\" + \\\n",
    "              photo['id'] + \"_\" + photo['secret'] + \".jpg\"\n",
    "        urls.append(url)\n",
    "    return urls\n",
    "\n",
    "\n",
    "\n",
    "# extact userid and albumid from Flickr album url (used to form image urls)\n",
    "def parse_url(url):\n",
    "\n",
    "    import re\n",
    "\n",
    "    try:\n",
    "        userid = re.search('photos/(.+?)/', url).group(1)\n",
    "    except AttributeError:\n",
    "        # AAA, ZZZ not found in the original string\n",
    "        userid = '' # apply your error handling\n",
    "\n",
    "    try:\n",
    "        albumid = re.search('albums/(.*)', url).group(1)\n",
    "    except AttributeError:\n",
    "        albumid = '' # apply your error handling\n",
    "\n",
    "    return userid, albumid\n",
    "\n",
    "\n",
    "def get_flickr_urls(url):\n",
    "\n",
    "    import flickrapi\n",
    "\n",
    "    #import flickr_keys\n",
    "    api_key = u'37528c980c419716e0879a417ef8211c'\n",
    "    api_secret = u'41075654a535c203'\n",
    "\n",
    "    # establish connection\n",
    "    flickr = flickrapi.FlickrAPI(api_key, api_secret, format='parsed-json')\n",
    "\n",
    "    # extract user and album id\n",
    "    userid, albumid = parse_url(url)\n",
    "\n",
    "    #fetch album info\n",
    "    albuminfo  = flickr.photosets.getPhotos(user_id=userid,photoset_id=albumid)\n",
    "\n",
    "    # extract individual photo urls\n",
    "    photo_urls = assemble_urls(albuminfo)\n",
    "\n",
    "    return photo_urls\n",
    "\n",
    "\n",
    "\n",
    "def get_photo_urls(url):\n",
    "    # input could be a Flickr photo album url\n",
    "    if 'www.flickr.com/photos/' in url:\n",
    "        photo_urls = get_flickr_urls(url)\n",
    "\n",
    "    # or a list of image jpeg urls, or even local filenames\n",
    "    else:\n",
    "        photo_urls = url.split(',')\n",
    "\n",
    "    return photo_urls\n",
    "\n",
    "\n",
    "\n",
    "# connect to google api\n",
    "def explicit(root_dir, api_dir, api_file):\n",
    "    from google.cloud import storage\n",
    "    # Explicitly use service account credentials by specifying the private key\n",
    "    # file.\n",
    "    storage_client = storage.Client.from_service_account_json(\n",
    "        root_dir + api_dir + '/' + api_file)\n",
    "\n",
    "    # Make an authenticated API request\n",
    "    buckets = list(storage_client.list_buckets())\n",
    "    print(buckets)\n",
    "\n",
    "\n",
    "\n",
    "def get_labels_for_images(photo_urls, root_dir, api_dir, api_file,image_location):\n",
    "    import os, io\n",
    "    from google.cloud import vision\n",
    "    from google.cloud.vision import types\n",
    "    import pandas as pd\n",
    "\n",
    "    # authenticate\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \\\n",
    "        root_dir+ api_dir + '/' + api_file\n",
    "    explicit(root_dir, api_dir, api_file)\n",
    "\n",
    "    # connect to Google api\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "    # feed photo url to Google, extract label\n",
    "    all_labels = []\n",
    "    for url in photo_urls:\n",
    "        # different syntax for remote and local images\n",
    "        if image_location == 'remote':\n",
    "            image = types.Image()\n",
    "            image.source.image_uri = url\n",
    "        elif image_location == 'local':\n",
    "            # open image file\n",
    "            with io.open(url, 'rb') as image_file:\n",
    "                content = image_file.read()\n",
    "            image = types.Image(content=content)\n",
    "        else:\n",
    "            return pd.DataFrame({'keywords':all_labels,'url':photo_urls})\n",
    "\n",
    "        # get and parse labels\n",
    "        response = client.label_detection(image=image)\n",
    "        labels = response.label_annotations\n",
    "        these_labels = ''\n",
    "        for label in labels:\n",
    "            these_labels += (label.description + ' ')\n",
    "        all_labels.append(these_labels)\n",
    "\n",
    "    # store labels as dataframe\n",
    "    df_all_labels = pd.DataFrame({'keywords':all_labels,'url':photo_urls})\n",
    "\n",
    "    # eliminate any photo that came back with zero labels\n",
    "    df_all_labels = df_all_labels.loc[df_all_labels.keywords.apply(lambda x: len(x))!=0]\n",
    "\n",
    "    return df_all_labels\n",
    "\n",
    "\n",
    "\n",
    "def extract_n_top_words_from_poem(poem_vector,feature_names):\n",
    "    import numpy as np\n",
    "\n",
    "    # adjust as necessary\n",
    "    ntopwords = 10\n",
    "\n",
    "    # rank keywords by tf-idf weight\n",
    "    indices = poem_vector.indices\n",
    "    rank_idx = poem_vector.data.argsort()[:-ntopwords:-1]\n",
    "\n",
    "    # form list of such words and return it, along with weights\n",
    "    keywords = [feature_names[indices[i]] for i in rank_idx]\n",
    "    weights = [poem_vector.data[i] for i in rank_idx]\n",
    "\n",
    "    return keywords, np.array(weights)\n",
    "\n",
    "\n",
    "# transform the image labels with the vectorizer\n",
    "def weight_labels(df_all_labels, vectorizer):\n",
    "    import spacy\n",
    "    import pandas as pd\n",
    "\n",
    "    image_words = []\n",
    "    image_weights = []\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # the vectorizer seems to need to have access to the parser, probably for the tokenizing step\n",
    "    parser = spacy.load('en')\n",
    "\n",
    "    for row in vectorizer.transform(df_all_labels['keywords'].tolist()):\n",
    "        kw, wt = extract_n_top_words_from_poem(row,feature_names)\n",
    "        image_words.append(kw)\n",
    "        image_weights.append(wt)\n",
    "\n",
    "    df_images = df_all_labels\n",
    "    df_images['keywords'] = image_words\n",
    "    df_images['weights'] = image_weights\n",
    "\n",
    "    # eliminate any photo that came back with zero labels\n",
    "    df_images = df_images.loc[df_images.keywords.apply(lambda x: len(x))!=0]\n",
    "\n",
    "    return df_images\n",
    "\n",
    "\n",
    "\n",
    "def images2vec(df_images):\n",
    "    import spacy\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # load parser, to be used with vectorizer\n",
    "    parser = spacy.load('en')\n",
    "\n",
    "    image_vectors = np.zeros((len(df_images),384))\n",
    "    j=0\n",
    "    for row in df_images.itertuples():\n",
    "        keywords = row.keywords\n",
    "        weights = row.weights\n",
    "        vecs = np.zeros((len(keywords),384))\n",
    "        i = 0\n",
    "        for k in keywords:\n",
    "            vecs[i,:] = parser(k).vector\n",
    "            i+=1\n",
    "        image_vectors[j,:]=np.dot(weights,vecs)\n",
    "        j+=1\n",
    "\n",
    "    return image_vectors\n",
    "\n",
    "\n",
    "\n",
    "def find_best_match(image_vectors, poem_vectors, image_sentiment, poem_sentiment,n_matches_per_photo=3,batch=False,lam=0.1,gamma=0.0):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    # find poem that maximizes a sentiment-regularized objective function\n",
    "    if batch:\n",
    "        image_vectors = np.mean(image_vectors,axis=0).reshape(1,384)\n",
    "        image_sentiment = [np.mean(image_sentiment)]\n",
    "\n",
    "    # assess the cosine similarity for each image/poem pair\n",
    "    sim = cosine_similarity(image_vectors,poem_vectors)\n",
    "\n",
    "    # also calculate the difference in sentiment score\n",
    "    dif = np.array([np.abs((im_s - poem_sentiment)) for im_s in image_sentiment])\n",
    "\n",
    "    # the net score is a weighted difference\n",
    "    net = sim - lam*dif\n",
    "\n",
    "    ix = net.argsort(axis=1)[:,:-n_matches_per_photo-1:-1]\n",
    "    scores = np.array([ list(net[i,ix[i,:]]) for i in range(len(ix))])\n",
    "    \n",
    "    return ix, scores\n",
    "\n",
    "\n",
    "\n",
    "def gather_results(ix,scores,df_images,df_poems,photo_urls):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    # gather top N poems (for each picture, or for the \"average\" picture)\n",
    "    results = pd.DataFrame([ df_poems.loc[ix[i,:],'poem'].tolist() for i in range(len(ix))],\\\n",
    "                           columns = [str(i) for i in range(1,ix.shape[1]+1)])\n",
    "\n",
    "    # collect image urls and keywords\n",
    "    if len(results) == len(df_images):\n",
    "        results[['url','keywords','weights','sentiment']] = \\\n",
    "        df_images[['url','keywords','weights','sentiment']]\n",
    "\n",
    "    # if in batchmode, collect images with the most keywords\n",
    "    else:\n",
    "        ix = np.argmax(df_images.keywords.apply(lambda x: len(x)))\n",
    "        results['url']= df_images.loc[ix,'url']\n",
    "        results['keywords']= [df_images.loc[ix,'keywords']]\n",
    "        results['weights']=[df_images.loc[ix,'weights']]\n",
    "        results['sentiment']= df_images.loc[ix,'sentiment']\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def ModelIt(url,image_location='remote', n_matches_per_photo = 3,batch=False,lam=0.1,gamma=0.0):\n",
    "\n",
    "    from PIL import Image, ImageDraw\n",
    "    import pandas as pd\n",
    "    import spacy\n",
    "    import numpy as np\n",
    "    from textblob import TextBlob\n",
    "\n",
    "    # load up path and file names, as well as runtime parameters\n",
    "    root_dir, api_dir, pkl_dir, api_file, poem_file, vec_file, vectorizer_file =\\\n",
    "        get_path_and_file_names()\n",
    "\n",
    "    # some of the larger data structures are stored in binary form, to expedite runtime\n",
    "    df_poems, df_vecs, vectorizer = get_pkl_files(root_dir,pkl_dir,poem_file,vec_file,vectorizer_file)\n",
    "    poem_vectors = df_vecs.values\n",
    "\n",
    "    # Set the variable \"photo_urls\", which is a list of urls of all images\n",
    "    photo_urls = get_photo_urls(url)\n",
    "\n",
    "    # Connect to Google-Cloud-Vision API and extract labels for each image\n",
    "    df_all_labels = get_labels_for_images(photo_urls, root_dir, api_dir, api_file, image_location)\n",
    "\n",
    "    # weight the keywords by the vectorizer used to process the poetry text\n",
    "    df_images = weight_labels(df_all_labels, vectorizer)\n",
    "\n",
    "    # append sentiment analysis for each image\n",
    "    df_images['sentiment'] = [TextBlob(' '.join(x)).sentiment[0] for x in df_images.keywords]\n",
    "\n",
    "    # if after extracting and weighting labels, nothing remains, exit gracefully\n",
    "    if len(df_images)==0:\n",
    "        return -1\n",
    "\n",
    "    # otherwise, embed image vectors via word2vec\n",
    "    image_vectors = images2vec(df_images)\n",
    "\n",
    "    # return sorted scores\n",
    "    ix, scores = find_best_match(image_vectors, poem_vectors, df_images['sentiment'], df_poems['sentiment'],batch=batch)\n",
    "\n",
    "    # gather all relevant info into a dataframe\n",
    "    results = gather_results(ix,scores,df_images,df_poems,photo_urls)\n",
    "\n",
    "    #pdb.set_trace()\n",
    "    # return a dictionary\n",
    "    return results.to_dict('records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Bucket: toews-images>]\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.flickr.com/photos/138072685@N02/albums/72157691244283801\"\n",
    "results = ModelIt(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'1': '\"Nature\" is what we see -- The Hill -- the Afternoon -- Squirrel -- Eclipse -- the Bumble bee -- Nay -- Nature is Heaven -- Nature is what we hear -- The Bobolink -- the Sea -- Thunder -- the Cricket -- Nay -- Nature is Harmony -- Nature is what we know -- Yet have no art to say -- So impotent Our Wisdom is To her Simplicity.',\n",
       "  '2': 'Nature assigns the Sun -- That -- is Astronomy -- Nature cannot enact a Friend -- That -- is Astrology.',\n",
       "  '3': 'My neighbor s daughter has created a city you cannot see on an island to which you cannot swim ruled by a noble princess and her athletic consort all the buildings are glass so that lies are impossible beneath the city they have buried certain words which can never be spoken again chiefly the word divorce which is eaten by maggots when it rains you hear chimes rabbits race through its suburbs the name of the city is one you can almost pronounce',\n",
       "  'keywords': ['forest', 'nature', 'autumn', 'path', 'tree'],\n",
       "  'sentiment': 0.0,\n",
       "  'url': 'https://farm5.staticflickr.com/4768/26128578578_eb800efd2f.jpg',\n",
       "  'weights': array([0.63937903, 0.54289974, 0.35405364, 0.34851714, 0.22279175])},\n",
       " {'1': \"The truth I do not stretch or shove When I state that the dog is full of love. I've also found, by actual test, A wet dog is the lovingest. \",\n",
       "  '2': 'Yesterday s newspaper becomes last week s Newspapers spread out like a hand-held fan In front of the face of the apartment Door. A dog does the Argos-thing inside, Waiting beside O as though his body Is but an Ithaca waiting the soul s Return. Neil the Super will soon come up With the key but only in time to find Doreen, the on-the-down-low-friend-with-perks, There already, kneeling between the two, Stroking the hair of both O and the dog, Wondering who had been walking the dog.',\n",
       "  '3': 'after Wolfgang Amadeus Mozart In the wobbly pirouette between song & dust, dog-nosed living room windows & a purple couch that should have been curbed last July: Saturday sunlight cuts it all every time you lean into some kind of ballet pose. Your belly & knobby elbow & leotarded knee wavering in a slim balance.  Jet , effac    I don t know what they mean & nod anyway. You reach & spin & dog hair hangs in the air like the start of heartfelt applause.',\n",
       "  'keywords': ['dog', 'like'],\n",
       "  'sentiment': 0.0,\n",
       "  'url': 'https://farm5.staticflickr.com/4648/39433614454_f332fc5b12.jpg',\n",
       "  'weights': array([0.99515803, 0.09828783])}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"/Users/ctoews/Documents/Insight/Project/photos/roads-diverging_test.jpg\"\n",
    "#results2 = ModelIt(image_path,image_location='local')\n",
    "image_location = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    600\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;31m# otherwise it looks like a programming error was the cause.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;31m# sending a valid response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             raise RemoteDisconnected(\"Remote end closed connection without\"\n\u001b[0m\u001b[1;32m    267\u001b[0m                                      \" response\")\n",
      "\u001b[0;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    638\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 639\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    640\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    600\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;31m# otherwise it looks like a programming error was the cause.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;31m# sending a valid response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             raise RemoteDisconnected(\"Remote end closed connection without\"\n\u001b[0m\u001b[1;32m    267\u001b[0m                                      \" response\")\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-6febd304ae5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Set the variable \"photo_urls\", which is a list of urls of all images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mphoto_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_photo_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Connect to Google-Cloud-Vision API and extract labels for each image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-5a241c3fe1d9>\u001b[0m in \u001b[0;36mget_photo_urls\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# input could be a Flickr photo album url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'www.flickr.com/photos/'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mphoto_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_flickr_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;31m# or a list of image jpeg urls, or even local filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-5a241c3fe1d9>\u001b[0m in \u001b[0;36mget_flickr_urls\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m#fetch album info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0malbuminfo\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mflickr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphotosets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetPhotos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muserid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphotoset_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malbumid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;31m# extract individual photo urls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/site-packages/flickrapi/call_builder.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflickrapi_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_flickr_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/site-packages/flickrapi/core.py\u001b[0m in \u001b[0;36mdo_flickr_call\u001b[0;34m(self, _method_name, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m                                     \u001b[0mparse_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'format'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m                                     **params)\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_supply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/site-packages/flickrapi/core.py\u001b[0m in \u001b[0;36m_wrap_in_parser\u001b[0;34m(self, wrapped_method, parse_format, *args, **kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mLOG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Wrapping call %s(self, %s, %s)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwrapped_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;31m# Just return if we have no parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/site-packages/flickrapi/core.py\u001b[0m in \u001b[0;36m_flickr_call\u001b[0;34m(self, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflickr_oauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREST_URL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;31m# Store in cache, if we have one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/site-packages/flickrapi/auth.py\u001b[0m in \u001b[0;36mdo_request\u001b[0;34m(self, url, params, timeout)\u001b[0m\n\u001b[1;32m    251\u001b[0m                                 \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                                 \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                                 timeout=timeout or self.default_timeout)\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;31m# check the response headers / status code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \"\"\"\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'POST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    506\u001b[0m         }\n\u001b[1;32m    507\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/insight/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))"
     ]
    }
   ],
   "source": [
    "image_location='remote' \n",
    "n_matches_per_photo = 3\n",
    "batch=False\n",
    "lam=0.1\n",
    "gamma=0.0\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "\n",
    "# load up path and file names, as well as runtime parameters\n",
    "root_dir, api_dir, pkl_dir, api_file, poem_file, vec_file, vectorizer_file =\\\n",
    "    get_path_and_file_names()\n",
    "\n",
    "# some of the larger data structures are stored in binary form, to expedite runtime\n",
    "df_poems, df_vecs, vectorizer = get_pkl_files(root_dir,pkl_dir,poem_file,vec_file,vectorizer_file)\n",
    "poem_vectors = df_vecs.values\n",
    "\n",
    "# Set the variable \"photo_urls\", which is a list of urls of all images\n",
    "photo_urls = get_photo_urls(url)\n",
    "\n",
    "# Connect to Google-Cloud-Vision API and extract labels for each image\n",
    "df_all_labels = get_labels_for_images(photo_urls, root_dir, api_dir, api_file, image_location)\n",
    "\n",
    "# weight the keywords by the vectorizer used to process the poetry text\n",
    "df_images = weight_labels(df_all_labels, vectorizer)\n",
    "\n",
    "# append sentiment analysis for each image\n",
    "df_images['sentiment'] = [TextBlob(' '.join(x)).sentiment[0] for x in df_images.keywords]\n",
    "\n",
    "# if after extracting and weighting labels, nothing remains, exit gracefully\n",
    "if len(df_images)==0:\n",
    "    return -1\n",
    "\n",
    "# otherwise, embed image vectors via word2vec\n",
    "image_vectors = images2vec(df_images)\n",
    "\n",
    "# return sorted scores\n",
    "ix, scores = find_best_match(image_vectors, poem_vectors, df_images['sentiment'], df_poems['sentiment'],batch=batch)\n",
    "\n",
    "# gather all relevant info into a dataframe\n",
    "results = gather_results(ix,scores,df_images,df_poems,photo_urls)\n",
    "\n",
    "#pdb.set_trace()\n",
    "# return a dictionary\n",
    "return results.to_dict('records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[585, 360, 4001], [2719, 3247, 3236]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_keys = ['1','2','3']\n",
    "ix = [[df_poems.loc[df_poems.poem==results[i][key]].index[0] for key in poem_keys] for i in [0,1]]\n",
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex1 = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex2_image=df_ex1.loc[1,['keywords','sentiment','weights']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex2_poems = df_poems.iloc[ix[1],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keywords                                   [dog, like]\n",
       "sentiment                                            0\n",
       "weights      [0.9951580293529281, 0.09828782536202801]\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ex2_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>poem</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>keywords</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2719</th>\n",
       "      <td>Ogden Nash</td>\n",
       "      <td>The truth I do not stretch or shove When I sta...</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>[dog, test, state, stretch, wet, truth, love]</td>\n",
       "      <td>[0.6147142694012745, 0.3695667852259223, 0.351...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247</th>\n",
       "      <td>Rowan Ricardo Phillips</td>\n",
       "      <td>Yesterday s newspaper becomes last week s News...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>[dog, wait, week, key, yesterday, spread, soon...</td>\n",
       "      <td>[0.5572529827948403, 0.3109458903811767, 0.221...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3236</th>\n",
       "      <td>Adrian Matejka</td>\n",
       "      <td>after Wolfgang Amadeus Mozart In the wobbly pi...</td>\n",
       "      <td>0.071875</td>\n",
       "      <td>[dog, nod, belly, nose, sunlight, knee, living...</td>\n",
       "      <td>[0.39774979938022664, 0.24090738384399552, 0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      author  \\\n",
       "2719              Ogden Nash   \n",
       "3247  Rowan Ricardo Phillips   \n",
       "3236          Adrian Matejka   \n",
       "\n",
       "                                                   poem  sentiment  \\\n",
       "2719  The truth I do not stretch or shove When I sta...   0.187500   \n",
       "3247  Yesterday s newspaper becomes last week s News...   0.066667   \n",
       "3236  after Wolfgang Amadeus Mozart In the wobbly pi...   0.071875   \n",
       "\n",
       "                                               keywords  \\\n",
       "2719      [dog, test, state, stretch, wet, truth, love]   \n",
       "3247  [dog, wait, week, key, yesterday, spread, soon...   \n",
       "3236  [dog, nod, belly, nose, sunlight, knee, living...   \n",
       "\n",
       "                                                weights  \n",
       "2719  [0.6147142694012745, 0.3695667852259223, 0.351...  \n",
       "3247  [0.5572529827948403, 0.3109458903811767, 0.221...  \n",
       "3236  [0.39774979938022664, 0.24090738384399552, 0.2...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ex2_poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for index, row in df_ex1_poems.iterrows():\n",
    "    #print(row['weights'])\n",
    "    #print(\"{:06.2f}\".format(row['weights'])\n",
    "    \n",
    "df_ex1_poems.weights = [[\"{:.2f}\".format(num) for num in df_ex1_poems.weights.values[i]] for i in range(len(df_ex1_poems))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>poem</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>keywords</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>Emily Dickinson</td>\n",
       "      <td>\"Nature\" is what we see -- The Hill -- the Aft...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[nature, cricket, thunder, wisdom, art, aftern...</td>\n",
       "      <td>[0.80, 0.23, 0.22, 0.22, 0.19, 0.19, 0.17, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>Emily Dickinson</td>\n",
       "      <td>Nature assigns the Sun -- That -- is Astronomy...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[nature, friend, sun]</td>\n",
       "      <td>[0.84, 0.44, 0.33]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>Alicia Ostriker</td>\n",
       "      <td>My neighbor s daughter has created a city you ...</td>\n",
       "      <td>0.049206</td>\n",
       "      <td>[city, word, building, island, create, daughte...</td>\n",
       "      <td>[0.57, 0.29, 0.23, 0.23, 0.23, 0.23, 0.22, 0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                               poem  \\\n",
       "585   Emily Dickinson  \"Nature\" is what we see -- The Hill -- the Aft...   \n",
       "360   Emily Dickinson  Nature assigns the Sun -- That -- is Astronomy...   \n",
       "4001  Alicia Ostriker  My neighbor s daughter has created a city you ...   \n",
       "\n",
       "      sentiment                                           keywords  \\\n",
       "585    0.000000  [nature, cricket, thunder, wisdom, art, aftern...   \n",
       "360    0.000000                              [nature, friend, sun]   \n",
       "4001   0.049206  [city, word, building, island, create, daughte...   \n",
       "\n",
       "                                                weights  \n",
       "585   [0.80, 0.23, 0.22, 0.22, 0.19, 0.19, 0.17, 0.1...  \n",
       "360                                  [0.84, 0.44, 0.33]  \n",
       "4001  [0.57, 0.29, 0.23, 0.23, 0.23, 0.23, 0.22, 0.2...  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ex1_poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "df_ex1_image.to_pickle('df_ex1_image.pkl')\n",
    "df_ex1_poems.to_pickle('df_ex1_poems.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_txtlist = [ df_poems.loc[ix[i,:],'poem'].tolist() for i in range(len(ix))]\n",
    "poem_txtcols = ['p_'+str(i) for i in range(1,ix.shape[1]+1)]\n",
    "poem_kwlist = [ df_poems.loc[ix[i,:],'keywords'].tolist() for i in range(len(ix))]\n",
    "poem_kwcols = ['p_kw_'+str(i) for i in range(1,ix.shape[1]+1)]\n",
    "poem_wtlist = [ df_poems.loc[ix[i,:],'weights'].tolist() for i in range(len(ix))]\n",
    "poem_wtcols = ['p_wt_'+str(i) for i in range(1,ix.shape[1]+1)]\n",
    "poem_sntmtlist = [ df_poems.loc[ix[i,:],'sentiment'].tolist() for i in range(len(ix))]\n",
    "poem_sntmtcols = ['p_st_'+str(i) for i in range(1,ix.shape[1]+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>poem</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>keywords</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>Emily Dickinson</td>\n",
       "      <td>\"Nature\" is what we see -- The Hill -- the Aft...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[nature, cricket, thunder, wisdom, art, aftern...</td>\n",
       "      <td>[0.80, 0.23, 0.22, 0.22, 0.19, 0.19, 0.17, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>Emily Dickinson</td>\n",
       "      <td>Nature assigns the Sun -- That -- is Astronomy...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[nature, friend, sun]</td>\n",
       "      <td>[0.84, 0.44, 0.33]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>Alicia Ostriker</td>\n",
       "      <td>My neighbor s daughter has created a city you ...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>[city, word, building, island, create, daughte...</td>\n",
       "      <td>[0.57, 0.29, 0.23, 0.23, 0.23, 0.23, 0.22, 0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                               poem  \\\n",
       "585   Emily Dickinson  \"Nature\" is what we see -- The Hill -- the Aft...   \n",
       "360   Emily Dickinson  Nature assigns the Sun -- That -- is Astronomy...   \n",
       "4001  Alicia Ostriker  My neighbor s daughter has created a city you ...   \n",
       "\n",
       "     sentiment                                           keywords  \\\n",
       "585       0.00  [nature, cricket, thunder, wisdom, art, aftern...   \n",
       "360       0.00                              [nature, friend, sun]   \n",
       "4001      0.05  [city, word, building, island, create, daughte...   \n",
       "\n",
       "                                                weights  \n",
       "585   [0.80, 0.23, 0.22, 0.22, 0.19, 0.19, 0.17, 0.1...  \n",
       "360                                  [0.84, 0.44, 0.33]  \n",
       "4001  [0.57, 0.29, 0.23, 0.23, 0.23, 0.23, 0.22, 0.2...  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ex1_poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>poem</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>keywords</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>Emily Dickinson</td>\n",
       "      <td>\"Nature\" is what we see -- The Hill -- the Aft...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[nature, cricket, thunder, wisdom, art, aftern...</td>\n",
       "      <td>[0.80, 0.23, 0.22, 0.22, 0.19, 0.19, 0.17, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>Emily Dickinson</td>\n",
       "      <td>Nature assigns the Sun -- That -- is Astronomy...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[nature, friend, sun]</td>\n",
       "      <td>[0.84, 0.44, 0.33]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>Alicia Ostriker</td>\n",
       "      <td>My neighbor s daughter has created a city you ...</td>\n",
       "      <td>0.049206</td>\n",
       "      <td>[city, word, building, island, create, daughte...</td>\n",
       "      <td>[0.57, 0.29, 0.23, 0.23, 0.23, 0.23, 0.22, 0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                               poem  \\\n",
       "585   Emily Dickinson  \"Nature\" is what we see -- The Hill -- the Aft...   \n",
       "360   Emily Dickinson  Nature assigns the Sun -- That -- is Astronomy...   \n",
       "4001  Alicia Ostriker  My neighbor s daughter has created a city you ...   \n",
       "\n",
       "      sentiment                                           keywords  \\\n",
       "585    0.000000  [nature, cricket, thunder, wisdom, art, aftern...   \n",
       "360    0.000000                              [nature, friend, sun]   \n",
       "4001   0.049206  [city, word, building, island, create, daughte...   \n",
       "\n",
       "                                                weights  \n",
       "585   [0.80, 0.23, 0.22, 0.22, 0.19, 0.19, 0.17, 0.1...  \n",
       "360                                  [0.84, 0.44, 0.33]  \n",
       "4001  [0.57, 0.29, 0.23, 0.23, 0.23, 0.23, 0.22, 0.2...  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ex1_poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_floats_to_strings(df):\n",
    "    #df.weights = [[\"{:.2f}\".format(num) for num in df.weights.values[i]] \\\n",
    "    #              for i in range(len(df))]\n",
    "    df.sentiment = [\"{:.2f}\".format(num) for num in df.sentiment.values]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_floats_to_strings(df_ex1_image)\n",
    "df_ex1_image.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex1_image=pd.read_pickle('df_ex1_image.pkl')\n",
    "df_ex1_poems=pd.read_pickle('df_ex1_poems.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               author                                               poem  \\\n",
      "585   Emily Dickinson  \"Nature\" is what we see -- The Hill -- the Aft...   \n",
      "360   Emily Dickinson  Nature assigns the Sun -- That -- is Astronomy...   \n",
      "4001  Alicia Ostriker  My neighbor s daughter has created a city you ...   \n",
      "\n",
      "     sentiment                                           keywords  \\\n",
      "585       0.00  [nature, cricket, thunder, wisdom, art, aftern...   \n",
      "360       0.00                              [nature, friend, sun]   \n",
      "4001      0.05  [city, word, building, island, create, daughte...   \n",
      "\n",
      "                                                weights  \n",
      "585   [0.80, 0.23, 0.22, 0.22, 0.19, 0.19, 0.17, 0.1...  \n",
      "360                                  [0.84, 0.44, 0.33]  \n",
      "4001  [0.57, 0.29, 0.23, 0.23, 0.23, 0.23, 0.22, 0.2...  \n",
      "boo!\n",
      "                      author  \\\n",
      "2719              Ogden Nash   \n",
      "3247  Rowan Ricardo Phillips   \n",
      "3236          Adrian Matejka   \n",
      "\n",
      "                                                   poem sentiment  \\\n",
      "2719  The truth I do not stretch or shove When I sta...      0.19   \n",
      "3247  Yesterday s newspaper becomes last week s News...      0.07   \n",
      "3236  after Wolfgang Amadeus Mozart In the wobbly pi...      0.07   \n",
      "\n",
      "                                               keywords  \\\n",
      "2719      [dog, test, state, stretch, wet, truth, love]   \n",
      "3247  [dog, wait, week, key, yesterday, spread, soon...   \n",
      "3236  [dog, nod, belly, nose, sunlight, knee, living...   \n",
      "\n",
      "                                                weights  \n",
      "2719         [0.61, 0.37, 0.35, 0.35, 0.32, 0.32, 0.19]  \n",
      "3247  [0.56, 0.31, 0.22, 0.22, 0.21, 0.20, 0.18, 0.1...  \n",
      "3236  [0.40, 0.24, 0.23, 0.23, 0.22, 0.22, 0.22, 0.2...  \n",
      "boo!\n"
     ]
    }
   ],
   "source": [
    "for set_ in (df_ex1_poems,df_ex2_poems):\n",
    "    print(set_)\n",
    "    print('boo!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.random?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.random.randint(0,5,size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "X = encoder.fit(test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_fit_transform',\n",
       " '_get_param_names',\n",
       " '_transform',\n",
       " 'active_features_',\n",
       " 'categorical_features',\n",
       " 'dtype',\n",
       " 'feature_indices_',\n",
       " 'fit',\n",
       " 'fit_transform',\n",
       " 'get_params',\n",
       " 'handle_unknown',\n",
       " 'n_values',\n",
       " 'n_values_',\n",
       " 'set_params',\n",
       " 'sparse',\n",
       " 'transform']"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.transform(test.reshape(-1,1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.active_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
